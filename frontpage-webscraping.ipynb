{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fa1ad08b",
   "metadata": {},
   "source": [
    "# Reddit Front Page (PH) Web Scraping and Data Visualization\n",
    "This program will mine the popular posts in reddit front page for Philippine users. Web scraping will be divided into two parts:\n",
    "\n",
    "    1. (No API) Listing out a number (say 100) of popular posts in Philippines. \n",
    "    2. (API) Gathering the important details of every post including title, subreddit, category, upvotes, comments, etc.\n",
    "After web scraping, the data will be saved in a JSON flat file which will be visualized below."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "79c8a13f",
   "metadata": {},
   "source": [
    "### Required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d04b875b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in ./venv/lib/python3.10/site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in ./venv/lib/python3.10/site-packages (from bs4) (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./venv/lib/python3.10/site-packages (from beautifulsoup4->bs4) (2.4.1)\n",
      "Requirement already satisfied: requests in ./venv/lib/python3.10/site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.10/site-packages (from requests) (3.1.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.10/site-packages (from requests) (2.0.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.10/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.10/site-packages (from requests) (2023.5.7)\n",
      "Requirement already satisfied: lxml in ./venv/lib/python3.10/site-packages (4.9.2)\n",
      "Requirement already satisfied: markdown in ./venv/lib/python3.10/site-packages (3.4.3)\n"
     ]
    }
   ],
   "source": [
    "%pip install bs4\n",
    "%pip install requests\n",
    "%pip install lxml\n",
    "%pip install markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f69720d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.parse\n",
    "import time\n",
    "import logging\n",
    "from markdown import markdown\n",
    "import concurrent.futures"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ce5738f2",
   "metadata": {},
   "source": [
    "## WEB SCRAPING"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3fce7241",
   "metadata": {},
   "source": [
    "### (a) List out the posts\n",
    "The target is to get the links for the posts in reddit.com/r/popular/?geo_filter=PH. Unfortunately, there is no API for this yet. So what I did is create `send_request` function that will send HTTP requests at most 3 times with error handling like Timeout error to pause the program for 10 seconds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b3a79e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send HTTP request\n",
    "def send_request(link, **kwargs):\n",
    "    # Try to send request 3 times\n",
    "    for attempt in range(3):\n",
    "        try:            \n",
    "            logging.debug(\"Sending request number %s of %s on %s\", attempt, 3, link)\n",
    "            r = requests.get(link, **kwargs)\n",
    "        except requests.exceptions.Timeout as e:\n",
    "            logging.warning(\"Timeout ERROR: Attempt number %s of %s failed. Sleeping for 10 seconds %s\\n\", attempt, retries, e)\n",
    "            time.sleep(10)\n",
    "            continue\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            # Something went wrong with the request. Covers other unspecified errors like ConnectionError and TooManyRedirects\n",
    "            logging.error(\"ERROR: Request failed. %s. Now exiting the application\", e)\n",
    "            raise SystemExit()\n",
    "\n",
    "        # If the request proceeds without error, break the loop and return the response\n",
    "        break\n",
    "    else:\n",
    "        # Exit the app if all retries are exhausted\n",
    "        logging.error(\"ERROR: All %s attemps of HTTP request on link %s failed. Now exiting the application\", 3, link)\n",
    "        raise SystemExit()\n",
    "\n",
    "    # If the code continues to run, it means that the request successfully went through.\n",
    "    logging.debug(\"SUCCESS: Request granted with status code of %s\", r.status_code)\n",
    "    return r"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5c7bb2e3",
   "metadata": {},
   "source": [
    "##### Proxy Rotation\n",
    "I also added proxy rotation to my requests to prevent my IP from getting banned after continuous requests. Reddit heavily limits bot requests to their server that does not use their official API. I used scraperapi.com service for this which gives 1000 free calls every month. The API key for `scraperapi` is stored in local JSON file (credentials.json) alongside other sensitive information like Reddit username, password, token, and client ID that will be used in the 2nd part of web scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c7e7f3cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Credentials in local JSON file\n",
    "    ## username = Reddit account username | password = Reddit account password\n",
    "    ## client_id and secret_token at https://www.reddit.com/prefs/apps after you created a script app\n",
    "    ## proxy_api is the API key after you created a free account in scraperapi.com\n",
    "    \n",
    "with open(\"credentials.json\", \"r\") as f:\n",
    "    credentials = json.load(f)\n",
    "\n",
    "## Use Proxy rotation since we are not yet using the API in this section\n",
    "params = {\n",
    "    'api_key': credentials[\"proxy_api\"],\n",
    "    'url': \"https://old.reddit.com/r/popular/?geo_filter=PH\"\n",
    "}\n",
    "\n",
    "post_links = set()\n",
    "\n",
    "## Scrape the first 4 pages of Reddit Front-page (PH region). \n",
    "for page in range(4):\n",
    "    # Send the request\n",
    "    r = send_request('http://api.scraperapi.com', params = params)\n",
    "    \n",
    "    # Parse the response\n",
    "    html = BeautifulSoup(r.text, \"lxml\")\n",
    "    \n",
    "    for post in html.select('#siteTable :not(.promoted):has(div.entry.unvoted)'):\n",
    "        post_link = post.get(\"data-permalink\")\n",
    "        \n",
    "        if post_link not in post_links and post_link:\n",
    "            post_links.add(post_link)\n",
    "    \n",
    "    logging.info(\"Posts in page %s gathered. Proceeding to page %s\", page + 1, page + 2)\n",
    "    # Change the link to the next page\n",
    "    next_page = html.select_one(\".next-button a\")[\"href\"]\n",
    "    params['url'] = next_page"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6f6778e6",
   "metadata": {},
   "source": [
    "### (b) Use Reddit API to get post details\n",
    "Now, we've got the links for the popular posts in Reddit PH front-page. The next step is to send request to each one of them to get the important details we need. This time we are using the reddit API, so we will no longer need proxy rotation. Access the credentials in the JSON files. We need this to get the OAuth token from reddit to authorize my requests. It will then be added to the header of every HTTP request on Reddit API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9d7d4d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "auth = requests.auth.HTTPBasicAuth(credentials['client_id'], credentials['secret_token'])\n",
    "account = {\n",
    "    \"grant_type\": \"password\",\n",
    "    \"username\": credentials[\"username\"],\n",
    "    \"password\": credentials[\"password\"]\n",
    "}\n",
    "\n",
    "# App info\n",
    "headers = {'User-Agent': 'Reddit Front Page Scrape Bot V1.0 by darren-sm'}\n",
    "\n",
    "# Request for oauth token\n",
    "r = requests.post('https://www.reddit.com/api/v1/access_token', data = account, auth = auth, headers = headers)\n",
    "\n",
    "# Add token to the header\n",
    "token = r.json()['access_token']\n",
    "headers['Authorization'] =  f\"bearer {token}\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2bf35898",
   "metadata": {},
   "source": [
    "##### Process the Data from the Reddit Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "eaaa0481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the comments in a thread\n",
    "def get_comments(comment_list):\n",
    "    for comment in comment_list:\n",
    "        data = comment[\"data\"]\n",
    "        body = data.get(\"body\")\n",
    "        if body:    \n",
    "            html = markdown(body)\n",
    "            yield ''.join(BeautifulSoup(html).findAll(string=True))            \n",
    "        \n",
    "        replies = data.get(\"replies\")\n",
    "        if isinstance(replies, dict):\n",
    "            yield from get_comments(replies[\"data\"][\"children\"])\n",
    "            \n",
    "def get_post_data(endpoint):\n",
    "    link = \"https://oauth.reddit.com\" + endpoint\n",
    "    r = send_request(link, headers = headers)\n",
    "    \n",
    "    # Basic Post Data\n",
    "    post_data = {\n",
    "        \"title\": r.json()[0]['data']['children'][0]['data']['title'],\n",
    "        \"text\": r.json()[0]['data']['children'][0]['data']['selftext'],\n",
    "        \"subreddit\": r.json()[0]['data']['children'][0]['data']['subreddit'],\n",
    "        \"upvotes\": r.json()[0]['data']['children'][0]['data']['score'],\n",
    "        \"date\": r.json()[0]['data']['children'][0]['data']['created'],\n",
    "        \"over_18\": r.json()[0]['data']['children'][0]['data']['over_18']\n",
    "    }\n",
    "\n",
    "    # Post Category\n",
    "    category = r.json()[0]['data']['children'][0]['data'].get('post_hint', None)\n",
    "    if category: \n",
    "        post_data[\"category\"] = category\n",
    "    else:\n",
    "        post_data[\"category\"] = \"text\"\n",
    "    \n",
    "    # Post comments (200 max)\n",
    "    post_data[\"comments\"] = list(get_comments(r.json()[1][\"data\"][\"children\"]))\n",
    "    \n",
    "    return post_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "259b6a4a",
   "metadata": {},
   "source": [
    "##### Multi-threading\n",
    "Make the program faster by sending asynchronous requests to Reddit. There's also a limit on how much requests we can send per minute or second but `send_request` method has error handling in case we passed over the limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e90759aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "front_page_data = []\n",
    "\n",
    "# Use multi-threading to send requests\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    for result in executor.map(get_post_data, post_links):\n",
    "        front_page_data.append(result)\n",
    "        \n",
    "# Save in one gigantic JSON file\n",
    "with open(\"front-page.json\", 'w') as f:\n",
    "    json.dump(front_page_data, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "58883e64",
   "metadata": {},
   "source": [
    "## DATA VISUALIZATION\n",
    "Our data is now set! Proceed to data visualization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3a6ed930",
   "metadata": {},
   "source": [
    "#### 1. Word Cloud\n",
    "Word cloud based on the comments of Reddit users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273d28b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "be86d9bc",
   "metadata": {},
   "source": [
    "#### 2. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
